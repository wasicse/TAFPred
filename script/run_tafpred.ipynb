{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Author: Md Wasi Ul Kabir\n",
    "# Date: December 28 2020\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import pathlib \n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import docker\n",
    "\n",
    "# check if any files are missing\n",
    "def checkmissing(source_dir):\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(\"Missing\", source_dir )\n",
    "        missing_list.append(pid)\n",
    "        \n",
    "# check if datafarme has NAN values    \n",
    "def checkforNAN(df,seqlength):\n",
    "    if df.isnull().values.any():\n",
    "        print(\"NAN\")\n",
    "        print(df[df.isnull().any(axis=1)])     \n",
    "   \n",
    "    if df.shape[0]!=seqlength:\n",
    "        print(\"Length Mismatch\", df.shape[0], seqlength)\n",
    "\n",
    "# merge all features into one file     \n",
    "def mergedata(PrintP,label):\n",
    "    id_list =open(\"../Features/id_list.txt\" ,\"r\")\n",
    "    output_dir =\"../output/merge_features/\"\n",
    "    output_file_end=\".csv\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    fasta_dir=\"../Features/FASTA/\"\n",
    "        \n",
    "    missing_list=[]\n",
    "    flag=0\n",
    "    for pid in id_list:\n",
    "        # print(pid)\n",
    "        pid=pid.strip()\n",
    "        output_file_name=output_dir+pid+output_file_end\n",
    "        \n",
    "        read_fasta= open(fasta_dir+pid+\".fasta\", \"r\")\n",
    "        fasta=read_fasta.readline()\n",
    "        fasta=read_fasta.readline()\n",
    "        # print(type(fasta))\n",
    "        seqlength=fasta[:-1].__len__()\n",
    "        print(pid, seqlength)   \n",
    "        #Dispredict Features      \n",
    "\n",
    "        source_dir=\"../Features/Features/DisPredict/Features/\"+pid+\"/\"+pid+\".57pfeatures\"                     \n",
    "        dispredict_column=['O/D(1)', 'AA(1)', 'PP(1)', 'PP(2)', 'PP(3)', 'PP(4)', 'PP(5)', 'PP(6)', 'PP(7)', 'PSSM(1)', 'PSSM(2)', 'PSSM(3)', 'PSSM(4)', 'PSSM(5)', 'PSSM(6)', 'PSSM(7)', 'PSSM(8)', 'PSSM(9)', 'PSSM(10)', 'PSSM(11)', 'PSSM(12)', 'PSSM(13)', 'PSSM(14)', 'PSSM(15)', 'PSSM(16)', 'PSSM(17)', 'PSSM(18)', 'PSSM(19)', 'PSSM(20)', 'SS(1)', 'SS(2)', 'SS(3)', 'ASA(1)', 'dphi(1)', 'dpsi(1)', 'MG(1)', 'BG(1)', 'BG(2)', 'BG(3)', 'BG(4)', 'BG(5)', 'BG(6)', 'BG(7)', 'BG(8)', 'BG(9)', 'BG(10)', 'BG(11)', 'BG(12)', 'BG(13)', 'BG(14)', 'BG(15)', 'BG(16)', 'BG(17)', 'BG(18)', 'BG(19)', 'BG(20)', 'sPSEE(1)', 't(1)']\n",
    "        df1=pd.read_csv(source_dir,delim_whitespace=True,header=None,skiprows=1 )\n",
    "        df1.columns=dispredict_column\n",
    "        drop_col= ['O/D(1)']\n",
    "        df1=df1.drop(drop_col, axis=1)\n",
    "        df1.columns=[\"Dis_\"+ s for s in  df1.columns]\n",
    "        checkforNAN(df1,seqlength)\n",
    "        if PrintP: print(\"Dispredict\", df1.shape)  \n",
    "                \n",
    "        # #PSSM Features    \n",
    "        source_dir=\"../Features/Features/PSSM_Parse/\"+pid+\".csv\"  \n",
    "\n",
    "        df3=pd.read_csv(source_dir )\n",
    "        df3.columns=[\"PSI_\"+ s for s in  df3.columns]\n",
    "        checkforNAN(df3,seqlength)\n",
    "        if PrintP: print(\"PSSM\",df3.shape)\n",
    "        \n",
    "        #Spider Features\n",
    "        source_dir=\"../Features/Features/Spider/\"+pid.strip()+ \".i1\"\n",
    "\n",
    "        df=pd.read_csv(source_dir,delim_whitespace=True)  \n",
    "        df4=df.drop([ \"#\",\"AA\",\"SS\",\"SS8\"], axis=1)\n",
    "        df4.columns=[\"Spi_\"+ s for s in  df4.columns]\n",
    "        checkforNAN(df4,seqlength)\n",
    "        if PrintP: print(\"Spider\",df4.shape)\n",
    "\n",
    "        \n",
    "        # #OPAL Features\n",
    "        source_dir=\"../Features/Features/OPAL/\"+pid+\".txt\"\n",
    "        df=pd.read_csv(source_dir,delim_whitespace=True )   \n",
    "        df5=df.drop([ \"No:\",\"residues\"], axis=1)\n",
    "        df5.columns=[\"Opa_\"+ s for s in  df5.columns]\n",
    "        checkforNAN(df5,seqlength)\n",
    "        if PrintP: print(\"OPAL\",df5.shape)\n",
    "\n",
    "        # #CNCC Features\n",
    "        source_dir=\"../Features/Features/CNCC/\"+pid+\".csv\"\n",
    "        df6=pd.read_csv(source_dir)  \n",
    "        checkforNAN(df6,seqlength)\n",
    "        if PrintP: print(\"CNCC\",df6.shape)\n",
    "        \n",
    "        \n",
    "        #SpotDisorder\n",
    "        source_dir=\"../Features/Features/SpotDisorder/\"+pid+\".csv\"   \n",
    "        col= (   \n",
    "            r\"ASA,HSEa-u,HSEa-d,CN13,theta,tau,phi,psi,theta_c,tau_c,phi_c,psi_c,P(3-C),P(3-E),P(3-H),P(8-C),P(8-S),P(8-T),P(8-H),\"\n",
    "            r\"P(8-G),P(8-I),P(8-E),P(8-B),HH_1,HH_2,HH_3,HH_4,HH_5,HH_6,HH_7,HH_8,HH_9,HH_10,HH_11,HH_12,HH_13,HH_14,HH_15,HH_16,\"\n",
    "            r\"HH_17,HH_18,HH_19,HH_20,HH_21,HH_22,HH_23,HH_24,HH_25,HH_26,HH_27,HH_28,HH_29,HH_30,PSSMS_1,PSSMS_2,PSSMS_3,PSSMS_4,\"\n",
    "            r\"PSSMS_5,PSSMS_6,PSSMS_7,PSSMS_8,PSSMS_9,PSSMS_10,PSSMS_11,PSSMS_12,PSSMS_13,PSSMS_14,PSSMS_15,PSSMS_16,PSSMS_17,PSSMS_18,PSSMS_19,PSSMS_20\"\n",
    "            )\n",
    "        SpotDisorder_column=col.split(\",\")  \n",
    "        SpotDisorder_column=[x.strip() for x in SpotDisorder_column ]\n",
    "        df9=pd.read_csv(source_dir,index_col=0 )\n",
    "        df9.columns=SpotDisorder_column\n",
    "        checkforNAN(df9,seqlength)        \n",
    "        if PrintP: print(\"SpotDisorder\",df9.shape)  \n",
    "        \n",
    "        #SpotDisorder Probability\n",
    "        source_dir=\"../Features/Features/SpotDisorderProba/\"+pid+\".spotd2\"    \n",
    "        df=pd.read_csv(source_dir,delim_whitespace=True,skiprows=1)  \n",
    "        df10=df.drop([ \"#\",\"AA\",\"O/D\"], axis=1)\n",
    "        df10.columns=[\"Spot_\"+ s for s in  df10.columns]\n",
    "        checkforNAN(df10,seqlength)\n",
    "        if PrintP: print(\"SpotDisorderProba\",df10.shape)\n",
    "\n",
    "    # Target (Set Dummy Target for windowing code)    \n",
    "        df0=pd.DataFrame()\n",
    "        df0[\"Target\"]=  range(df10.shape[0])\n",
    "        print(\"Target\",df0.shape)\n",
    "\n",
    "    # code to add target\n",
    "        # target_dir=\"../Feature_Extraction/Dataset/example/\"+Angle+\"target/\"\n",
    "        # target_dir_file_end=\".csv\"\n",
    "        # source_dir=target_dir+pid+target_dir_file_end\n",
    "        # df0=pd.read_csv(source_dir,index_col=0)   #,header=None\n",
    "    \n",
    "        # # df0.columns=[\"Target\"]\n",
    "        # if PrintP: print(\"Target\",df0.shape)\n",
    "\n",
    "        listdf=[df0,df10,df4,df3,df5,df6,df9,df1,df4[[\"Spi_Phi\",\"Spi_Psi\"]]]\n",
    "        merged = pd.concat(listdf, axis=1)\n",
    "        merged = merged.loc[:, ~merged.columns.str.contains('^Unnamed')] \n",
    "        if PrintP: print(\"Merge\",merged.shape)  \n",
    "\n",
    "\n",
    "        for dff in listdf:      \n",
    "            if(dff.isnull().values.any()):\n",
    "                prinf(dff)\n",
    "                print(\"NAN\")\n",
    "                break;\n",
    "            \n",
    "        if(merged.isnull().values.any()):\n",
    "            print(\"Merge files have NAN\")\n",
    "            \n",
    "        merged.to_csv(output_dir+\"/\"+pid+'.csv',index=False,header=label) \n",
    "  \n",
    "    return merged.shape\n",
    "\n",
    "# run windowing code\n",
    "def windowing(len,startwindow_size,endwindow_size):\n",
    "    \n",
    "    windowintput=open(\"./window_param.txt\",\"w\")    \n",
    "    windowintput.write(len+ '\\n')\n",
    "    windowintput.write(str(startwindow_size)+ '\\n')\n",
    "    windowintput.write(str(endwindow_size)+ '\\n')\n",
    "    windowintput.close()\n",
    "    output_dir =\"../output/Windowed_file/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    bashCommand='javac run_windowing.java '\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    bashCommand='java run_windowing'\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    print(output)\n",
    "\n",
    "\n",
    "def getprediction():\n",
    "    np.set_printoptions(precision=3)\n",
    "    workspace=\"../output\"\n",
    "    pathlib.Path(workspace).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    X_test=pd.read_csv(\"../output/Windowed_file/feat_179_w_3.csv\",header=None).to_numpy()\n",
    "    X_test=X_test[:,1:]\n",
    "\n",
    "    feature=open(\"../models/OPTGeneticFeat.txt\",\"r\").readline()\n",
    "    mask=pd.DataFrame(list(feature)).astype(int)[0].to_numpy().astype(bool)\n",
    "    fmask=[]   \n",
    "    for i in range(3):\n",
    "        fmask.extend(mask)\n",
    "\n",
    "    X_test = X_test[:, fmask]\n",
    "\n",
    "\n",
    "    scaler= joblib.load(\"../models/phi_scaler.pkl\")\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    phisaved_model = joblib.load(\"../models/phi_model.pkl\")\n",
    "    phiproba = phisaved_model.predict(X_test)\n",
    "\n",
    "    psisaved_model = joblib.load(\"../models/psi_model.pkl\")\n",
    "    psiproba = psisaved_model.predict(X_test)\n",
    "    print(phiproba.shape)\n",
    "\n",
    "    id_list =open(\"../Dataset/example/id_list.txt\" ,\"r\")\n",
    "    start=0\n",
    "    for pid in id_list:\n",
    "        pid=pid.strip()\n",
    "        fastafile=open(\"../Dataset/example/FASTA/\"+pid+\".fasta\",\"r\")\n",
    "        fasta=fastafile.readline().strip()\n",
    "        fasta=fastafile.readline().strip()    \n",
    "        end=start+fasta.__len__()\n",
    "   \n",
    "        print(\"S\",start,\"E\",end)\n",
    "        fphiproba=phiproba[start:end]\n",
    "        print(fphiproba.shape)\n",
    "        fpsiproba=psiproba[start:end]    \n",
    "        result=np.hstack((  np.round(np.arange(1,fasta.__len__()+1).reshape(-1,1),0) ,np.round(fphiproba, 3).reshape(-1,1) ,np.round(fpsiproba, 3).reshape(-1,1) )) \n",
    "        with open(\"../output/\"+pid+\"_result.txt\", \"w\") as f:\n",
    "            f.write(\"Residue No\\tΔPhi\\tΔPsi\\n\")\n",
    "        with open(\"../output/\"+pid+\"_result.txt\", \"ab\") as f:\n",
    "            np.savetxt(f,  result, delimiter='\\t\\t',fmt=\"%s \")        \n",
    "        start=fasta.__len__()\n",
    "\n",
    "# check if a container is running or not\n",
    "def checkcontainerstatus(containername):\n",
    "    cli = docker.APIClient()\n",
    "    try:\n",
    "        inspect_dict = cli.inspect_container(containername)\n",
    "        state = inspect_dict['State']\n",
    "        print(state)\n",
    "        is_running = state['Status'] == 'running'\n",
    "\n",
    "        if is_running:\n",
    "            print(\"My container is running!\")\n",
    "            return False\n",
    "    except:\n",
    "        print(\"My container is not running!\")\n",
    "        return True\n",
    "\n",
    "# collect features from docker container\n",
    "def collectfeatures(containername):\n",
    "\n",
    "\n",
    "    print(\"Pulling docker image\")\n",
    "    bashCommand=\"docker pull wasicse/featureextract:1.0\"\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    print(output)\n",
    "\n",
    "    print(\"Checking container status\")\n",
    "    bashCommand=\"docker ps -q -f name={\"+containername+\"}\"\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    print(output)\n",
    "    if checkcontainerstatus(containername):\n",
    "        print(\"Creating container\")\n",
    "        bashCommand=\"docker run -itd -v \"+os.getcwd()+\"/Databases:/opt/common --name \"+containername+\"  wasicse/featureextract:1.0\"\n",
    "        print(bashCommand)\n",
    "        output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "        print(output)\n",
    "    else:\n",
    "        print(\"Starting container\")\n",
    "        bashCommand=\"docker start \"+containername\n",
    "        print(bashCommand)\n",
    "        output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "        print(output)   \n",
    "\n",
    "    print(\"Copying files to container\")\n",
    "    bashCommand=\"docker cp ../Dataset/example/ \"+containername+\":/opt/FeatureExtractionDocker/Dataset\"\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    print(output)\n",
    "\n",
    "    print(\"Removing old features\")\n",
    "    bashCommand=\"rm -rf ../Features/\"\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    print(output)\n",
    "\n",
    "    print(\"Running feature extraction. It might take a while to complete. and depends on the number of sequences in the input file.\")\n",
    "    bashCommand= \"docker exec \"+containername+\" bash -c \\\"cd /opt/FeatureExtractionDocker/FeatureExtractTool ; /opt/.pyenv/versions/miniconda3-3.9-4.10.3/envs/feat/bin/python runScript.py\\\"\"\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    print(output)\n",
    "    \n",
    "    print(\"copying features from container\")\n",
    "    bashCommand=\"docker cp \"+containername+\":/opt/FeatureExtractionDocker/Dataset/example/ ../Features/\"\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    print(output)\n",
    "\n",
    "    # print(\"Stop container\")\n",
    "    # bashCommand=\"docker stop \"+containername\n",
    "    # print(bashCommand)\n",
    "    # output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    # print(output)  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    parent_path = str(Path(__file__).resolve().parents[1])\n",
    "    print(\"Parent Dir\",parent_path)\n",
    "\n",
    "    parser = OptionParser()\n",
    "    parser.add_option(\"-f\", \"--containerName\", dest=\"containerName\", help=\"Container Name.\", default=\"taffeaturesv1\")\n",
    "    parser.add_option(\"-o\", \"--output_path\", dest=\"output_path\", help=\"Path to output.\", default=parent_path+'/output/')\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "\n",
    "    print(\"Container Name:\",options.containerName)\n",
    "    print(\"Output Path:\",options.output_path)\n",
    "\n",
    "    #Print features options\n",
    "    PrintP=True\n",
    "    #Add label to features\n",
    "    label=True\n",
    "\n",
    "    # Collect features by running the docker container\n",
    "    collectfeatures(options.containerName)\n",
    "\n",
    "    # Merge features\n",
    "    merge_shape=mergedata(PrintP,label)\n",
    "\n",
    "    # Windowing \n",
    "    startwindow_size=3\n",
    "    endwindow_size=3\n",
    "    windowing(str(merge_shape[1]-1),startwindow_size,endwindow_size)\n",
    "\n",
    "    # Get prediction from the windowed features\n",
    "    getprediction()\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Timeforcast.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
